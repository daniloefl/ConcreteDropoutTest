{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, absolute_import\n",
    "\n",
    "import gc\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "x_train, y_train = make_moons(n_samples = 5000, noise = 0.10)\n",
    "x_test, y_test = make_moons(n_samples = 2000, noise = 0.10)\n",
    "y_test = y_test.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f79e2d54c90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "#torch.cuda.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_gaussian_prob(x, mu, sigma, log_sigma = False):\n",
    "    if not log_sigma:\n",
    "        element_wise_log_prob = -0.5*torch.Tensor([np.log(2*np.pi)]).to(mu.device) - torch.log(sigma) - 0.5*(x-mu)**2 / sigma**2\n",
    "    else:\n",
    "        element_wise_log_prob = -0.5*torch.Tensor([np.log(2*np.pi)]).to(mu.device) - F.softplus(sigma) - 0.5*(x-mu)**2 / F.softplus(sigma)**2\n",
    "    return element_wise_log_prob.sum()\n",
    "\n",
    "class GaussianLinear(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, stddev_prior = 1.0, bias=True):\n",
    "        super(GaussianLinear, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.stddev_prior = stddev_prior\n",
    "        self.w_mu = nn.Parameter(torch.Tensor(in_dim, out_dim).normal_(0, stddev_prior))\n",
    "        self.w_rho = nn.Parameter(torch.Tensor(in_dim, out_dim).normal_(0, stddev_prior))\n",
    "        self.b_mu = nn.Parameter(torch.Tensor(out_dim).normal_(0, stddev_prior)) if bias else None\n",
    "        self.b_rho = nn.Parameter(torch.Tensor(out_dim).normal_(0, stddev_prior)) if bias else None\n",
    "        self.bias = bias\n",
    "        self.q_w = 0.\n",
    "        self.p_w = 0.\n",
    "\n",
    "    def forward(self, x, test=False):\n",
    "        if test:\n",
    "            w = self.w_mu\n",
    "            b = self.b_mu if self.bias else None\n",
    "        else:\n",
    "            device = self.w_mu.device\n",
    "            w_stddev = F.softplus(self.w_rho)\n",
    "            b_stddev = F.softplus(self.b_rho) if self.bias else None\n",
    "            w = self.w_mu + w_stddev * torch.Tensor(self.in_dim, self.out_dim).to(device).normal_(0,self.stddev_prior)\n",
    "            b = self.b_mu + b_stddev * torch.Tensor(self.out_dim).to(device).normal_(0,self.stddev_prior) if self.bias else None\n",
    "            self.q_w = log_gaussian_prob(w, self.w_mu, self.w_rho, log_sigma=True)\n",
    "            self.p_w = log_gaussian_prob(w, torch.zeros_like(self.w_mu, device=device), self.stddev_prior*torch.ones_like(w_stddev, device=device))\n",
    "            if self.bias:\n",
    "                self.q_w += log_gaussian_prob(b, self.b_mu, self.b_rho, log_sigma=True)\n",
    "                self.p_w += log_gaussian_prob(b, torch.zeros_like(self.b_mu, device=device), self.stddev_prior*torch.ones_like(b_stddev, device=device))\n",
    "        output = x@w+b\n",
    "        return output\n",
    "\n",
    "    def get_pw(self):\n",
    "        return self.p_w\n",
    "\n",
    "    def get_qw(self):\n",
    "        return self.q_w\n",
    "\n",
    "class ModelBNN(nn.Module):\n",
    "    def __init__(self, L, layers, stddev_prior = 1.0):\n",
    "        super(ModelBNN, self).__init__()\n",
    "        self.stddev_prior = stddev_prior\n",
    "        \n",
    "        self.layers = [L]+layers\n",
    "        self.linear = nn.ModuleList([])\n",
    "        for ii in range(1, len(self.layers)):\n",
    "            self.linear.append(GaussianLinear(self.layers[ii-1], self.layers[ii], stddev_prior, bias = True))\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, test=False):\n",
    "        y = x\n",
    "        for ii in range(1, len(self.layers)):\n",
    "            y = self.linear[ii-1](y, test)\n",
    "            if ii != len(self.layers)-1:\n",
    "                y = self.relu(y)\n",
    "        return y\n",
    "\n",
    "    def forward_samples(self, x, y, nb_samples=3):\n",
    "        total_qw, total_pw, total_log_likelihood = 0., 0., 0.\n",
    "        for _ in range(nb_samples):\n",
    "            output = self.forward(x)\n",
    "            total_qw += self.get_qw()\n",
    "            total_pw += self.get_pw()\n",
    "            total_log_likelihood += log_gaussian_prob(y, output, torch.ones_like(y))\n",
    "        return total_qw / nb_samples, total_pw / nb_samples, total_log_likelihood / nb_samples\n",
    "\n",
    "    def get_pw(self):\n",
    "        return sum([self.linear[ii-1].p_w for ii in range(1, len(self.layers))])\n",
    "\n",
    "    def get_qw(self):\n",
    "        return sum([self.linear[ii-1].q_w for ii in range(1, len(self.layers))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nepochs = 500\n",
    "layers = [100, 50, 5, 1]\n",
    "L = 2\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_fn(model, x, y):\n",
    "    return torch.pow(model(x)-y, 2).sum(1).mean(0)\n",
    "\n",
    "def fit_model(layers, Nepochs, x_train, y_train):\n",
    "    N = x_train.shape[0]\n",
    "    L = x_train.shape[1]\n",
    "    \n",
    "    model = ModelBNN(L = L, layers = layers, stddev_prior = 1.0)\n",
    "    #model = model.cuda()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "    for i in range(Nepochs):\n",
    "        old_batch = 0\n",
    "        loss_ = 0\n",
    "        nll_ = 0\n",
    "        kl_ = 0\n",
    "        mse_ = 0\n",
    "        Nbatches = int(np.ceil(N/batch_size))\n",
    "        pall = np.random.permutation(N)\n",
    "        for batch in range(Nbatches):\n",
    "            #p = np.random.choice(N, batch_size)\n",
    "            p = pall[batch*batch_size : (batch+1)*batch_size]\n",
    "            _x = x_train[p, :].astype(np.float32)\n",
    "            _y = y_train[p, np.newaxis].astype(np.float32)\n",
    "            \n",
    "            x = torch.as_tensor(_x) #.cuda()\n",
    "            y = torch.as_tensor(_y) #.cuda()\n",
    "            \n",
    "            qw, pw, llh = model.forward_samples(x, y)\n",
    "            kl_loss = (qw-pw)/len(y)\n",
    "            nll_loss = -llh/len(y)\n",
    "            loss_total = nll_loss + kl_loss\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss_total.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            mse = mse_fn(model, x, y)\n",
    "            \n",
    "            loss_ += loss_total.item()\n",
    "            nll_ += nll_loss.item()\n",
    "            kl_ += kl_loss.item()\n",
    "            mse_ += mse.item()\n",
    "            print(\"Epoch {:>3d}, {:^12s}: loss = {:>5.4f}, nll = {:>5.4f}, kl = {:>5.4f}, mse = {:>5.4f}\".format(i,\n",
    "                                        \"{:>3d}/{:>3d}\".format(batch+1, Nbatches),\n",
    "                                        (loss_)/float(batch+1), nll_/float(batch+1), kl_/float(batch+1),\n",
    "                                        mse_/float(batch+1)\n",
    "                                        ),\n",
    "                  \n",
    "                  end = '\\r')\n",
    "\n",
    "\n",
    "        loss_ /= float(Nbatches)\n",
    "        nll_ /= float(Nbatches)\n",
    "        kl_ /= float(Nbatches)\n",
    "        mse_ /= float(Nbatches)\n",
    "        print(\"Epoch {:>3d}, {:^12s}: loss = {:>5.4f}, nll = {:>5.4f}, kl = {:>5.4f}, mse = {:>5.4f}\".format(i,\n",
    "                                        \"\",\n",
    "                                        (loss_), nll_, kl_,\n",
    "                                        mse_\n",
    "                                        ),\n",
    "                  \n",
    "                  end = '\\n')            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0,             : loss = 12869.9625, nll = 12964.9841, kl = -95.0216, mse = 21567.7964\n",
      "Epoch   1,             : loss = 4093.6338, nll = 4188.7968, kl = -95.1630, mse = 12190.7034\n",
      "Epoch   2,             : loss = 1843.2898, nll = 1938.7241, kl = -95.4343, mse = 4349.7344\n",
      "Epoch   3,             : loss = 1167.7464, nll = 1263.0289, kl = -95.2825, mse = 1940.59025\n",
      "Epoch   4,             : loss = 637.2631, nll = 732.7223, kl = -95.4592, mse = 1208.3812\n",
      "Epoch   5,             : loss = 367.1667, nll = 462.4606, kl = -95.2939, mse = 1129.486701\n",
      "Epoch   6,             : loss = 158.7009, nll = 254.5199, kl = -95.8190, mse = 321.3978\n",
      "Epoch   7,             : loss = 157.0631, nll = 252.7120, kl = -95.6489, mse = 400.34025\n",
      "Epoch   8,             : loss = 80.6436, nll = 176.6459, kl = -96.0023, mse = 871.097747\n",
      "Epoch   9,             : loss = 61.2992, nll = 157.2392, kl = -95.9400, mse = 286.97871\n",
      "Epoch  10,             : loss = -4.8319, nll = 91.2997, kl = -96.1315, mse = 317.468972\n",
      "Epoch  11,             : loss = -27.9414, nll = 68.1795, kl = -96.1209, mse = 70.28562\n",
      "Epoch  12,             : loss = -43.4846, nll = 53.3390, kl = -96.8236, mse = 124.8209\n",
      "Epoch  13,             : loss = -61.0273, nll = 35.9953, kl = -97.0226, mse = 103.3932\n",
      "Epoch  14,             : loss = -46.0577, nll = 51.2422, kl = -97.2998, mse = 145.7658\n",
      "Epoch  15,             : loss = -38.8585, nll = 58.9374, kl = -97.7959, mse = 119.9568\n",
      "Epoch  16,             : loss = -82.4559, nll = 15.5728, kl = -98.0286, mse = 49.37445\n",
      "Epoch  17,             : loss = -79.4776, nll = 18.9290, kl = -98.4066, mse = 54.2343\n",
      "Epoch  18,             : loss = -83.3608, nll = 15.8737, kl = -99.2345, mse = 57.0333\n",
      "Epoch  19,             : loss = -74.5959, nll = 25.2225, kl = -99.8184, mse = 14.9378\n",
      "Epoch  20,             : loss = -88.6065, nll = 11.9949, kl = -100.6014, mse = 22.4835\n",
      "Epoch  21,             : loss = -79.4477, nll = 21.4876, kl = -100.9353, mse = 6.9390\n",
      "Epoch  22,             : loss = -87.7670, nll = 14.1429, kl = -101.9099, mse = 26.9662\n",
      "Epoch  23,             : loss = -98.2250, nll = 4.4283, kl = -102.6534, mse = 16.7247\n",
      "Epoch  24,             : loss = -95.2381, nll = 8.6630, kl = -103.9011, mse = 6.14133\n",
      "Epoch  25,             : loss = -98.5772, nll = 6.5118, kl = -105.0890, mse = 4.41676\n",
      "Epoch  26,             : loss = -101.7365, nll = 4.2632, kl = -105.9998, mse = 13.2829\n",
      "Epoch  27,             : loss = -101.8591, nll = 5.3445, kl = -107.2035, mse = 5.3381\n",
      "Epoch  28,             : loss = -83.9906, nll = 24.5608, kl = -108.5514, mse = 9.64688\n",
      "Epoch  29,             : loss = -103.1081, nll = 7.4499, kl = -110.5580, mse = 141.72744\n",
      "Epoch  30,             : loss = -108.1303, nll = 3.6554, kl = -111.7856, mse = 2.1414\n",
      "Epoch  31,             : loss = -108.2604, nll = 5.0867, kl = -113.3471, mse = 5.7527\n",
      "Epoch  32,             : loss = -112.9830, nll = 2.6254, kl = -115.6085, mse = 1.9083\n",
      "Epoch  33,             : loss = -115.3300, nll = 2.2000, kl = -117.5300, mse = 1.4281\n",
      "Epoch  34,             : loss = -118.0060, nll = 1.9000, kl = -119.9060, mse = 1.1110\n",
      "Epoch  35,             : loss = -118.7679, nll = 3.4596, kl = -122.2275, mse = 1.2824\n",
      "Epoch  36,             : loss = -123.2939, nll = 1.5889, kl = -124.8828, mse = 2.13983\n",
      "Epoch  37,             : loss = -121.8219, nll = 5.5626, kl = -127.3846, mse = 5.944191\n",
      "Epoch  38,             : loss = -127.1828, nll = 3.2832, kl = -130.4660, mse = 0.8073\n",
      "Epoch  39,             : loss = -131.5392, nll = 1.4565, kl = -132.9957, mse = 0.5895\n",
      "Epoch  40,             : loss = -135.1324, nll = 1.4533, kl = -136.5857, mse = 0.5748\n",
      "Epoch  41,             : loss = -134.9301, nll = 4.6112, kl = -139.5413, mse = 0.88027\n",
      "Epoch  42,             : loss = -141.8088, nll = 1.2676, kl = -143.0764, mse = 0.4897\n",
      "Epoch  43,             : loss = -144.1918, nll = 2.4390, kl = -146.6308, mse = 0.48846\n",
      "Epoch  44,             : loss = -149.4270, nll = 1.1534, kl = -150.5804, mse = 0.4269\n",
      "Epoch  45,             : loss = -152.7432, nll = 1.3891, kl = -154.1323, mse = 16.5322\n",
      "Epoch  46,             : loss = -154.4513, nll = 3.6397, kl = -158.0910, mse = 1.52745\n",
      "Epoch  47,             : loss = -161.2521, nll = 1.2374, kl = -162.4895, mse = 0.3848\n",
      "Epoch  48,             : loss = -164.9323, nll = 1.8437, kl = -166.7760, mse = 0.3623\n",
      "Epoch  49,             : loss = -168.2783, nll = 2.9281, kl = -171.2064, mse = 0.47858\n",
      "Epoch  50,             : loss = -172.3274, nll = 2.9631, kl = -175.2905, mse = 0.6727\n",
      "Epoch  51,             : loss = -176.6982, nll = 2.5904, kl = -179.2887, mse = 0.3499\n",
      "Epoch  52,             : loss = -181.8007, nll = 1.3919, kl = -183.1926, mse = 0.3362\n",
      "Epoch  53,             : loss = -186.2006, nll = 1.1046, kl = -187.3052, mse = 0.4884\n",
      "Epoch  54,             : loss = -189.8888, nll = 1.4595, kl = -191.3483, mse = 0.3132\n",
      "Epoch  55,             : loss = -194.1338, nll = 1.1610, kl = -195.2949, mse = 0.3390\n",
      "Epoch  56,             : loss = -198.2821, nll = 1.0700, kl = -199.3520, mse = 0.3029\n",
      "Epoch  57,    17/250   : loss = -199.4534, nll = 1.5593, kl = -201.0127, mse = 0.3176\r"
     ]
    }
   ],
   "source": [
    "model = fit_model(layers, Nepochs, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMeanStd(model, x, Npred = 50):\n",
    "    # per-example predictions\n",
    "    p = [None]*Npred\n",
    "    x_ = Variable(torch.FloatTensor(x)) #.cuda()\n",
    "    for k in range(Npred):\n",
    "        p[k] = model(x_)\n",
    "        p[k] = p[k].detach().numpy()[np.newaxis,...,0]\n",
    "    p = np.concatenate(p, axis = 0)\n",
    "    return np.mean(p, axis = 0), np.std(p, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contour(model, x, y, getFunction):\n",
    "    # make countour\n",
    "    mins = [np.min(x[:,0]), np.min(x[:,1])]\n",
    "    maxs = [np.max(x[:,0]), np.max(x[:,1])]\n",
    "    step = [(maxs[0] - mins[0])/50.0, (maxs[1] - mins[1])/50.0]\n",
    "    bx, by = np.mgrid[mins[0]:(maxs[0]+0.5*step[0]):step[0], mins[1]:(maxs[1]+0.5*step[0]):step[1]]\n",
    "    inputs = np.vstack([bx.flatten(), by.flatten()]).T\n",
    "    inputs = inputs.astype(np.float32)\n",
    "\n",
    "    pred_m, pred_s = getFunction(model, inputs, Npred = 50)\n",
    "    pred_m_2d = pred_m.reshape( (-1, bx.shape[1]) )\n",
    "    pred_s_2d = pred_s.reshape( (-1, bx.shape[1]) )\n",
    "\n",
    "    # if one wants to smoothen the results\n",
    "    #for data in [pred_m_2d, pred_s_2d]:\n",
    "    #    data = gaussian_filter(data, 0.1)\n",
    "\n",
    "    fig, ax = plt.subplots(nrows = 2, ncols = 1, sharex = True, figsize = (10, 8))\n",
    "    cmap = sns.diverging_palette(250, 12, s=85, l=25, as_cmap=True)\n",
    "    contour_s = ax[0].contourf(bx, by, pred_s_2d, cmap = cmap)\n",
    "    cbar_s = plt.colorbar(contour_s, ax = ax[0])\n",
    "    cbar_s.ax.set_ylabel('Unc.')\n",
    "    contour_m = ax[1].contourf(bx, by, pred_m_2d, cmap = cmap)\n",
    "    cbar_m = plt.colorbar(contour_m, ax = ax[1])\n",
    "    cbar_m.ax.set_ylabel('Mean')\n",
    "    for a in [ax[0], ax[1]]:\n",
    "        a.scatter(x[y == 1,0], x[y == 1,1], color = 'r', marker = 's', s = 5, label = 'y = 1')\n",
    "        a.scatter(x[y == 0,0], x[y == 0,1], color = 'b', marker = 's', s = 5, label = 'y = 0')\n",
    "        a.set(xlabel = 'A', ylabel = 'B', title = '')\n",
    "        a.set_xlim([mins[0], maxs[0]])\n",
    "        a.set_ylim([mins[1], maxs[1]])\n",
    "        a.legend(frameon = True)\n",
    "    ax[0].set_xlabel('')\n",
    "    fig.subplots_adjust(hspace = 0)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-539de9b5f143>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_contour\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetFunction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetMeanStd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "plot_contour(model, x_test, y_test, getFunction = getMeanStd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
