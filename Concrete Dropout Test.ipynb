{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison between Concrete Dropout and Gaussian-prior BNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create two-moons dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "x_train, y_train = make_moons(n_samples = 5000, noise = 0.20)\n",
    "x_test, y_test = make_moons(n_samples = 2000, noise = 0.20)\n",
    "y_test = y_test.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model using Concrete Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.layers import Dense, Lambda, Wrapper\n",
    "from tensorflow.python.ops import random_ops\n",
    "\n",
    "# Keras layer wrapper to implement Concrete Dropout\n",
    "class ConcreteDropout(Wrapper):\n",
    "    \"\"\"This wrapper allows to learn the dropout probability for any given input Dense layer.\n",
    "    ```python\n",
    "        # as the first layer in a model\n",
    "        model = Sequential()\n",
    "        model.add(ConcreteDropout(Dense(8), input_shape=(16)))\n",
    "        # now model.output_shape == (None, 8)\n",
    "        # subsequent layers: no need for input_shape\n",
    "        model.add(ConcreteDropout(Dense(32)))\n",
    "        # now model.output_shape == (None, 32)\n",
    "    ```\n",
    "    `ConcreteDropout` can be used with arbitrary layers which have 2D\n",
    "    kernels, not just `Dense`. However, Conv2D layers require different\n",
    "    weighing of the regulariser (use SpatialConcreteDropout instead).\n",
    "    # Arguments\n",
    "        layer: a layer instance.\n",
    "        weight_regularizer:\n",
    "            A positive number which satisfies\n",
    "                $weight_regularizer = l**2 / (\\tau * N)$\n",
    "            with prior lengthscale l, model precision $\\tau$ (inverse observation noise),\n",
    "            and N the number of instances in the dataset.\n",
    "            Note that kernel_regularizer is not needed.\n",
    "        dropout_regularizer:\n",
    "            A positive number which satisfies\n",
    "                $dropout_regularizer = 2 / (\\tau * N)$\n",
    "            with model precision $\\tau$ (inverse observation noise) and N the number of\n",
    "            instances in the dataset.\n",
    "            Note the relation between dropout_regularizer and weight_regularizer:\n",
    "                $weight_regularizer / dropout_regularizer = l**2 / 2$\n",
    "            with prior lengthscale l. Note also that the factor of two should be\n",
    "            ignored for cross-entropy loss, and used only for the eculedian loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layer, weight_regularizer=1e-6, dropout_regularizer=1e-5,\n",
    "                 init_min=0.1, init_max=0.1, is_mc_dropout=True, **kwargs):\n",
    "        assert 'kernel_regularizer' not in kwargs\n",
    "        super(ConcreteDropout, self).__init__(layer, **kwargs)\n",
    "        self.weight_regularizer = weight_regularizer\n",
    "        self.dropout_regularizer = dropout_regularizer\n",
    "        self.is_mc_dropout = is_mc_dropout\n",
    "        self.supports_masking = True\n",
    "        self.p_logit = None\n",
    "        self.init_min = np.log(init_min) - np.log(1. - init_min)\n",
    "        self.init_max = np.log(init_max) - np.log(1. - init_max)\n",
    "\n",
    "    def build(self, input_shape=None):\n",
    "        if not self.layer.built:\n",
    "            self.layer.build(input_shape)\n",
    "            self.layer.built = True\n",
    "        super(ConcreteDropout, self).build()  # this is very weird.. we must call super before we add new losses\n",
    "\n",
    "        # initialise p\n",
    "        self.p_logit = self.layer.add_weight(name='p_logit',\n",
    "                                            shape=(1,),\n",
    "                                            initializer=initializers.RandomUniform(self.init_min, self.init_max),\n",
    "                                            trainable=True)\n",
    "\n",
    "        # initialise regulariser / prior KL term\n",
    "        assert len(input_shape) == 2, 'this wrapper only supports Dense layers'\n",
    "        self.input_dim = np.prod(input_shape[-1])  # we drop only last dim\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return self.layer.compute_output_shape(input_shape)\n",
    "\n",
    "    def concrete_dropout(self, x):\n",
    "        '''\n",
    "        Concrete dropout - used at training time (gradients can be propagated)\n",
    "        :param x: input\n",
    "        :return:  approx. dropped out input\n",
    "        '''\n",
    "        eps = K.cast_to_floatx(K.epsilon())\n",
    "        temp = 0.1\n",
    "\n",
    "        unif_noise = K.random_uniform(shape=K.shape(x))\n",
    "        #unif_noise = random_ops.random_uniform(shape = K.shape(x), dtype=x.dtype)\n",
    "        p = K.sigmoid(self.p_logit[0])\n",
    "        drop_prob = (\n",
    "            K.log(p + eps)\n",
    "            - K.log(1. - p + eps)\n",
    "            + K.log(unif_noise + eps)\n",
    "            - K.log(1. - unif_noise + eps)\n",
    "        )\n",
    "        drop_prob = K.sigmoid(drop_prob / temp)\n",
    "        random_tensor = 1. - drop_prob\n",
    "\n",
    "        retain_prob = 1. - p\n",
    "        x *= random_tensor\n",
    "        x /= retain_prob\n",
    "        return x\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        p = K.sigmoid(self.p_logit[0])\n",
    "        weight = self.layer.kernel\n",
    "        kernel_regularizer = self.weight_regularizer * K.sum(K.square(weight)) / (1. - p)\n",
    "        dropout_regularizer = p * K.log(p)\n",
    "        dropout_regularizer += (1. - p) * K.log(1. - p)\n",
    "        dropout_regularizer *= self.dropout_regularizer * self.input_dim\n",
    "        regularizer = K.sum(kernel_regularizer + dropout_regularizer)\n",
    "        self.layer.add_loss(regularizer)\n",
    "        if self.is_mc_dropout:\n",
    "            return self.layer.call(self.concrete_dropout(inputs))\n",
    "        else:\n",
    "            def relaxed_dropped_inputs():\n",
    "                return self.layer.call(self.concrete_dropout(inputs))\n",
    "            return K.in_train_phase(relaxed_dropped_inputs,\n",
    "                                    self.layer.call(inputs),\n",
    "                                    training=training)\n",
    "\n",
    "def makeModel(x_train, y_train, heteroscedatic = False, activation = tf.nn.relu):\n",
    "    batch_size = 20\n",
    "    Nepochs = 200\n",
    "    train_size = len(x_train)\n",
    "    L = x_train.shape[1]\n",
    "\n",
    "    lengthscale = 1e-4\n",
    "    wd = lengthscale**2/train_size\n",
    "    dd = 2./train_size\n",
    "    layers = [40, 10, 5]\n",
    "\n",
    "    x_in = tf.keras.layers.Input(shape = (L,))\n",
    "    x = x_in\n",
    "    for il, l in enumerate(layers):\n",
    "        x = ConcreteDropout(tf.keras.layers.Dense(l, activation = activation), weight_regularizer = wd, dropout_regularizer = dd)(x)\n",
    "    if heteroscedatic:\n",
    "        m = ConcreteDropout(tf.keras.layers.Dense(2, activation = None), weight_regularizer = wd, dropout_regularizer = dd)(x)\n",
    "    else:\n",
    "        m = ConcreteDropout(tf.keras.layers.Dense(1, activation = None), weight_regularizer = wd, dropout_regularizer = dd)(x)\n",
    "\n",
    "    def homoloss(y_true, y_pred):\n",
    "        mean_ = y_pred[:,0]\n",
    "        y_ = y_true[:,0]\n",
    "        return tf.square(y_ - mean_)\n",
    "    def heteroloss(y_true, y_pred):\n",
    "        mean_ = y_pred[:,0]\n",
    "        logvar_ = y_pred[:,1]\n",
    "        prec_ = tf.exp(-logvar_)\n",
    "        y_ = y_true[:,0]\n",
    "        return prec_*tf.square(y_ - mean_) + logvar_\n",
    "\n",
    "    model = tf.keras.Model(x_in, m)\n",
    "    if heteroscedatic:\n",
    "        model.compile(loss = heteroloss, optimizer = tf.keras.optimizers.Adam(1e-3), metrics = [])\n",
    "    else:\n",
    "        model.compile(loss = homoloss, optimizer = tf.keras.optimizers.Adam(1e-3), metrics = [])\n",
    "    history = model.fit(x_train, y_train,\n",
    "              epochs = Nepochs,\n",
    "              batch_size = batch_size,\n",
    "              shuffle = True,\n",
    "              validation_data = (x_test, y_test),\n",
    "              callbacks = [tf.keras.callbacks.EarlyStopping(monitor = 'loss',\n",
    "                                                            min_delta = 0,\n",
    "                                                            patience = 2,\n",
    "                                                            verbose = 1,\n",
    "                                                            mode = 'auto',\n",
    "                                                            restore_best_weights = True)])\n",
    "    p_list = [tf.nn.sigmoid(x) for x in model.trainable_weights if \"p_logit\" in x.name]\n",
    "    print(\"Dropout probabilities:\", p_list)\n",
    "    return model, history\n",
    "\n",
    "\n",
    "def getMeanStd(model, x_test, Npred = 50):\n",
    "    pred_m = np.zeros( (len(x_test),) )\n",
    "    pred_s = np.zeros( (len(x_test),) )\n",
    "    for i in range(0, Npred):\n",
    "        tmp = model.predict(x_test)[:,0]\n",
    "        pred_m += tmp\n",
    "        pred_s += tmp**2\n",
    "        del tmp\n",
    "        gc.collect()\n",
    "    pred_m /= float(Npred)\n",
    "    pred_s /= float(Npred)\n",
    "    pred_s -= pred_m**2\n",
    "    pred_s[pred_s <= 0] = 1e-6\n",
    "    pred_s = np.sqrt(pred_s)\n",
    "        \n",
    "    return pred_m, pred_s\n",
    "\n",
    "def getMeanStdH(model, x_test, Npred = 50):\n",
    "    pred_m = np.zeros( (len(x_test),) )\n",
    "    pred_s = np.zeros( (len(x_test),) )\n",
    "    pred_s_ale = 0.5*np.exp(model.predict(x_test)[:,1])\n",
    "    for i in range(0, Npred):\n",
    "        tmp = model.predict(x_test)[:,0]\n",
    "        pred_m += tmp\n",
    "        pred_s += tmp**2\n",
    "        del tmp\n",
    "        gc.collect()\n",
    "    pred_m /= float(Npred)\n",
    "    pred_s /= float(Npred)\n",
    "    pred_s -= pred_m**2\n",
    "    pred_s[pred_s <= 0] = 1e-6\n",
    "    pred_s = np.sqrt(pred_s)\n",
    "    pred_s = np.sqrt(pred_s_ale**2 + pred_s**2)\n",
    "        \n",
    "    return pred_m, pred_s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000 samples, validate on 2000 samples\n",
      "Epoch 1/200\n",
      "5000/5000 [==============================] - 12s 2ms/sample - loss: -0.5682 - val_loss: -0.9289\n",
      "Epoch 2/200\n",
      "5000/5000 [==============================] - 4s 737us/sample - loss: -1.0310 - val_loss: -1.0726\n",
      "Epoch 3/200\n",
      "5000/5000 [==============================] - 4s 868us/sample - loss: -1.1499 - val_loss: -1.1807\n",
      "Epoch 4/200\n",
      "5000/5000 [==============================] - 4s 761us/sample - loss: -1.2514 - val_loss: -1.2180\n",
      "Epoch 5/200\n",
      "5000/5000 [==============================] - 4s 774us/sample - loss: -1.3172 - val_loss: -1.3062\n",
      "Epoch 6/200\n",
      "5000/5000 [==============================] - 4s 751us/sample - loss: -1.3882 - val_loss: -1.3373\n",
      "Epoch 7/200\n",
      "5000/5000 [==============================] - 4s 788us/sample - loss: -1.4761 - val_loss: -1.4230\n",
      "Epoch 8/200\n",
      "5000/5000 [==============================] - 4s 834us/sample - loss: -1.5219 - val_loss: -1.5215\n",
      "Epoch 9/200\n",
      "5000/5000 [==============================] - 4s 785us/sample - loss: -1.6508 - val_loss: -1.7092\n",
      "Epoch 10/200\n",
      "5000/5000 [==============================] - 4s 834us/sample - loss: -1.8324 - val_loss: -1.7960\n",
      "Epoch 11/200\n",
      "5000/5000 [==============================] - 4s 729us/sample - loss: -1.9721 - val_loss: -1.9572\n",
      "Epoch 12/200\n",
      "5000/5000 [==============================] - 4s 754us/sample - loss: -2.1352 - val_loss: -1.9205\n",
      "Epoch 13/200\n",
      "5000/5000 [==============================] - 4s 774us/sample - loss: -2.3347 - val_loss: -2.0684\n",
      "Epoch 14/200\n",
      "5000/5000 [==============================] - 4s 812us/sample - loss: -2.3865 - val_loss: -2.2326\n",
      "Epoch 15/200\n",
      "5000/5000 [==============================] - 4s 776us/sample - loss: -2.4288 - val_loss: -2.2029\n",
      "Epoch 16/200\n",
      "5000/5000 [==============================] - 4s 819us/sample - loss: -2.4683 - val_loss: -2.3984\n",
      "Epoch 17/200\n",
      "5000/5000 [==============================] - 4s 735us/sample - loss: -2.5133 - val_loss: -2.3559\n",
      "Epoch 18/200\n",
      "5000/5000 [==============================] - 3s 596us/sample - loss: -2.5887 - val_loss: -2.4282\n",
      "Epoch 19/200\n",
      "5000/5000 [==============================] - 3s 638us/sample - loss: -2.7108 - val_loss: -2.5066\n",
      "Epoch 20/200\n",
      "5000/5000 [==============================] - 4s 713us/sample - loss: -2.8282 - val_loss: -2.5354\n",
      "Epoch 21/200\n",
      "2160/5000 [===========>..................] - ETA: 1s - loss: -2.7865"
     ]
    }
   ],
   "source": [
    "hetero_model, hetero_model_h = makeModel(x_train, y_train, heteroscedatic = True, activation = tf.nn.relu)\n",
    "homo_model, homo_model_h = makeModel(x_train, y_train, heteroscedatic = False, activation = tf.nn.relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Gaussian-prior BNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def posterior_mean_field(kernel_size, bias_size=0, dtype=None):\n",
    "    n = kernel_size + bias_size\n",
    "    c = np.log(np.exp(1e-3)-1.0)\n",
    "    return tf.keras.Sequential([tfp.layers.VariableLayer(2*n, initializer = 'zeros', trainable = True, dtype=dtype),\n",
    "                       tfp.layers.DistributionLambda(lambda t: tfp.distributions.Independent(\n",
    "                           tfp.distributions.Normal(loc=t[..., :n], scale=1e-4+tf.math.softplus(c + 1e-4*t[..., n:])),\n",
    "                           reinterpreted_batch_ndims=1))\n",
    "    ])\n",
    "\n",
    "def prior_fixed(kernel_size, bias_size=0, dtype=None):\n",
    "    n = kernel_size + bias_size\n",
    "    return tf.keras.Sequential([tfp.layers.VariableLayer(n, initializer = 'zeros', trainable = False, dtype=dtype),\n",
    "                                tfp.layers.DistributionLambda(lambda t: tfp.distributions.Independent(\n",
    "                                tfp.distributions.Normal(loc = t, scale = 1.0), reinterpreted_batch_ndims = 1))\n",
    "                                ])\n",
    "\n",
    "def makeModelBNN(x_train, y_train, heteroscedatic = False, activation = tf.nn.relu):\n",
    "    batch_size = 20\n",
    "    Nepochs = 200\n",
    "    train_size = len(x_train)\n",
    "    L = x_train.shape[1]\n",
    "\n",
    "    layers = [40, 10, 5]\n",
    "\n",
    "    x_in = tf.keras.layers.Input(shape = (L,))\n",
    "    x = x_in\n",
    "    for il, l in enumerate(layers):\n",
    "        x = tfp.layers.DenseVariational(l,\n",
    "                                        posterior_mean_field,\n",
    "                                        prior_fixed,\n",
    "                                        kl_weight = 1.0/train_size,\n",
    "                                        kl_use_exact = True,\n",
    "                                        activation = activation,\n",
    "                                        )(x)\n",
    "    n_out = 1\n",
    "    if heteroscedatic:\n",
    "        n_out = 2\n",
    "    x = tfp.layers.DenseVariational(n_out,\n",
    "                                        posterior_mean_field,\n",
    "                                        prior_fixed,\n",
    "                                        kl_weight = 1.0/train_size,\n",
    "                                        kl_use_exact = True,\n",
    "                                        activation = None,\n",
    "                                        )(x)\n",
    "    if heteroscedatic:\n",
    "        c = np.log(np.exp(1e-3)-1.0)\n",
    "        x = tfp.layers.DistributionLambda(lambda t: tfp.distributions.Normal(loc = t[...,:1],\n",
    "                                                                    scale = 1e-4 + tf.math.softplus(c+1e-4*t[...,1:])\n",
    "                                                                        ))(x)\n",
    "    else:\n",
    "        x = tfp.layers.DistributionLambda(lambda t: tfp.distributions.Normal(loc = t,\n",
    "                                                                        scale = 1e-4\n",
    "                                                                        ))(x)\n",
    "\n",
    "    nll = lambda y_true, y_pred: -tf.reduce_mean(y_pred.log_prob(y_true))\n",
    "    model = tf.keras.Model(x_in, x)\n",
    "    model.compile(loss = nll, optimizer = tf.keras.optimizers.Adam(1e-3), metrics = [])\n",
    "    history = model.fit(x_train, y_train,\n",
    "              epochs = Nepochs,\n",
    "              batch_size = batch_size,\n",
    "              shuffle = True,\n",
    "              validation_data = (x_test, y_test),\n",
    "              callbacks = [tf.keras.callbacks.EarlyStopping(monitor = 'loss',\n",
    "                                                            min_delta = 0,\n",
    "                                                            patience = 2,\n",
    "                                                            verbose = 1,\n",
    "                                                            mode = 'auto',\n",
    "                                                            restore_best_weights = True)])\n",
    "    return model, history\n",
    "\n",
    "def getMeanStdBNN(model, x_test, Npred = 50):\n",
    "    pred_m = np.zeros( (len(x_test),) )\n",
    "    pred_s = np.zeros( (len(x_test),) )\n",
    "    for i in range(0, Npred):\n",
    "        tmp = model.predict(x_test)[:,0]\n",
    "        pred_m += tmp\n",
    "        pred_s += tmp**2\n",
    "        del tmp\n",
    "        gc.collect()\n",
    "    pred_m /= float(Npred)\n",
    "    pred_s /= float(Npred)\n",
    "    pred_s -= pred_m**2\n",
    "    pred_s[pred_s <= 0] = 1e-6\n",
    "    pred_s = np.sqrt(pred_s)\n",
    "    return pred_m, pred_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hetero_model_bnn, hetero_model_bnn_h = makeModelBNN(x_train, y_train, heteroscedatic = True, activation = tf.nn.relu)\n",
    "homo_model_bnn, homo_model_bnn_h = makeModelBNN(x_train, y_train, heteroscedatic = False, activation = tf.nn.relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to do plots showing mean and uncertainties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contour(model, x, y, getFunction):\n",
    "    # make countour\n",
    "    mins = [np.min(x[:,0]), np.min(x[:,1])]\n",
    "    maxs = [np.max(x[:,0]), np.max(x[:,1])]\n",
    "    step = [(maxs[0] - mins[0])/25.0, (maxs[1] - mins[1])/25.0]\n",
    "    bx, by = np.mgrid[mins[0]:(maxs[0]+0.5*step[0]):step[0], mins[1]:(maxs[1]+0.5*step[0]):step[1]]\n",
    "    inputs = np.vstack([bx.flatten(), by.flatten()]).T\n",
    "    inputs = inputs.astype(np.float32)\n",
    "\n",
    "    pred_m, pred_s = getFunction(model, inputs, Npred = 50)\n",
    "    pred_m_2d = pred_m.reshape( (-1, bx.shape[1]) )\n",
    "    pred_s_2d = pred_s.reshape( (-1, bx.shape[1]) )\n",
    "\n",
    "    # if one wants to smoothen the results\n",
    "    #for data in [pred_m_2d, pred_s_2d]:\n",
    "    #    data = gaussian_filter(data, 0.1)\n",
    "\n",
    "    fig, ax = plt.subplots(nrows = 2, ncols = 1, sharex = True, figsize = (10, 8))\n",
    "    cmap = sns.diverging_palette(250, 12, s=85, l=25, as_cmap=True)\n",
    "    contour_s = ax[0].contourf(bx, by, pred_s_2d, cmap = cmap)\n",
    "    cbar_s = plt.colorbar(contour_s, ax = ax[0])\n",
    "    cbar_s.ax.set_ylabel('Unc.')\n",
    "    contour_m = ax[1].contourf(bx, by, pred_m_2d, cmap = cmap)\n",
    "    cbar_m = plt.colorbar(contour_m, ax = ax[1])\n",
    "    cbar_m.ax.set_ylabel('Mean')\n",
    "    for a in [ax[0], ax[1]]:\n",
    "        a.scatter(x[y == 1,0], x[y == 1,1], color = 'r', marker = 's', s = 5, label = 'y = 1')\n",
    "        a.scatter(x[y == 0,0], x[y == 0,1], color = 'b', marker = 's', s = 5, label = 'y = 0')\n",
    "        a.set(xlabel = 'A', ylabel = 'B', title = '')\n",
    "        a.set_xlim([mins[0], maxs[0]])\n",
    "        a.set_ylim([mins[1], maxs[1]])\n",
    "        a.legend(frameon = True)\n",
    "    ax[0].set_xlabel('')\n",
    "    fig.subplots_adjust(hspace = 0)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Countour for heteroscedatic Concrete Dropout model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contour(hetero_model, x_test, y_test, getFunction = getMeanStdH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contour for homoscedatic Concrete Dropout model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contour(homo_model, x_test, y_test, getFunction = getMeanStd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contour for heteroscedatic Gaussian-prior BNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contour(hetero_model_bnn, x_test, y_test, getFunction = getMeanStdBNN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contour for homoscedatic Gaussian-prior BNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contour(homo_model_bnn, x_test, y_test, getFunction = getMeanStdBNN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
